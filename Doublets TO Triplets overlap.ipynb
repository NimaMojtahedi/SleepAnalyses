{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cd34bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "os.chdir('L:/analysis_nima/Python Scripts') #changing directory to ETL.py folder\n",
    "from Analyses_ import ETL\n",
    "from Analyses_ import findseq\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f9454b",
   "metadata": {},
   "source": [
    "### Input informations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb5278fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_address =['L:/analysis_nima/250520_m18a/Batch_4', 'L:/analysis_nima/290520_m17a/Batch/extracells/Batch_2',\n",
    "                'L:/analysis_nima/110620_m18b/new/Batch_3', 'L:/analysis_nima/050820_m21a/Batch_4',\n",
    "                'L:/analysis_nima/030720_m20a/Batch_3', 'L:/analysis_nima/291020_m23a/Batch_2', \n",
    "               'L:/analysis_nima/210121_m24a/Batch_1']\n",
    "\n",
    "# sampling rate for different mice\n",
    "sfs = [30.995, 30.995, 30.955, 30.995, 30.995, 30.9576, 30.995]\n",
    "\n",
    "# defining artifacts\n",
    "custom_artifacts_1 = [[11007,11008,4]]\n",
    "custom_artifacts_2 = [[11630,11947,0],[12000,12700,0],[4533,5154,14]]\n",
    "custom_artifacts_3 = []\n",
    "custom_artifacts_4 = [[1595,1845,0],[4609,4907,0],[7740,7983,0],[13795,14101,0],[16706,16973,0],\n",
    "             [7266,7501,1],[11912,12219,1],[12703,13203,1],[13730,13909,1],[15097,15559,1],[17277,17519,1],\n",
    "             [1416,1793,2],[3816,4123,2],[13254,13593,2],[15224,15703,2],[17668,18067,2],\n",
    "             [7358,7521,3],[10240,10253,3],[13672,13748,3],[17306,17593,3],\n",
    "             [5505,5939,4],[6222,6231,4],[16760,16820,4],[18450,18573,4],                           \n",
    "             [1132,1357,7],[8653,8776,7],[9946,10391,7],\n",
    "             [9068,9323,7],[13589,13623,7],[17394,17547,7],\n",
    "             [1,140,8],[908,965,8],[7638,7665,8],[8014,8027,8],[14726,14922,8],[18022,18187,8],\n",
    "             [1912,2381,9],[2920,2923,9],[7767,7819,9],[7864,7997,9],[9876,10197,9],[15206,15545,9],\n",
    "             [6734,7220,10],[9668,9939,10],\n",
    "             [8914,9290,11],[9562,9571,11],[12216,12339,11],\n",
    "             [11492,11553,12],[13898,13919,12],\n",
    "             [6298,6619,13],[12282,12795,13],[14406,14507,13],[14798,14875,13],\n",
    "             [1,220,14],[1878,2108,14],[9872,9961,14],\n",
    "             [4764,4813,15],[5564,5571,15],[15419,15595,15],[16003,16089,15],[16420,16482,15],\n",
    "             [4240,4403,16],[9975,10077,16],[10274,10275,16],[13228,13451,16],[13805,13939,16],[14620,14664,16],[15764,15857,16],[18430,18537,16],\n",
    "             [10425,10591,17],[11058,11709,17],[12232,12311,17],[12784,12804,17],[17000,17115,17],\n",
    "             [10655,10825,18],[11993,12075,18],[12648,12685,18],[16013,16134,18],\n",
    "             [14466,15015,19],[16876,17009,19],[18398,18573,19],\n",
    "            [628,958,23],[7822,7976,23],[10298,10435,23],[13899,14241,23],[14466,14537,23],[15773,15848,23],[17664,17925,23],[18291,18426,23],\n",
    "            [4309,4512,24],[5271,5467,24],[6450,6700,24],[7186,7593,24],\n",
    "            [460,700,25],[1714,1725,25],[4045,4153,25],[13600,13699,25],\n",
    "            [4434,4568,26],[5984,6167,26],\n",
    "            [1770,1831,27],[4060,4064,27],\n",
    "            [2000,2193,28],[11098,11334,28],\n",
    "            [5921,5997,30],[8200,8210,30],[9189,9191,30],\n",
    "            [686,852,31],[13375,13543,31],[14059,14334,31],[15808,15883,31],\n",
    "            [10760,10800,33],[13942,14061,33],[17672,17738,33],\n",
    "            [10576,10604,41]]\n",
    "custom_artifacts_5 = [[3928,3971,0],[4584,4656,0],[7001,7119,0],[14144,14230,0],[14466,14501,0],[15591,15593,0],\n",
    "            [559,603,1],[2187,2214,1],[3999,4054,1],[11749,11790,1],[17785,17839,1],\n",
    "            [3403,3491,2],[11936,11980,2],\n",
    "            [1397,1465,3],[13881,13889,3],[14510,14526,3],[18377,18441,3],                                                                                           \n",
    "            [2585,2635,4], [6722,6771,4],[10867,10897,4],[16543,16615,4],\n",
    "            [740,825,5],[2026,2113,5],[13025,13051,5],\n",
    "            [798,863,6],[1795,1857,6],[3949,3980,6],[5190,5244,6],[10480,10547,6],[11219,11301,6],[12144,12191,6],[12690,12747,6],\n",
    "            [1343,1441,7],[2618,2684,7],[2709,2713,7],[3427,3496,7],[4138,4185,7],[4532,4567,7],[5694,8768,7],[9513,9562,7],[12510,12644,7],[15000,15073,7],\n",
    "            [96,140,8],[4175,4258,8],\n",
    "            [409,564,9],[13291,13497,9],[16520,18573,9],\n",
    "            [9968,10026,10],[14269,14335,10],[16569,16616,10],\n",
    "            [404,557,11],[12553,12649,11],\n",
    "            [503,598,13],[2832,2889,13],[13151,13218,13],[18276,18343,13],\n",
    "            [769,954,14],[980,1001,14],[2324,2389,14],[4286,4297,14],\n",
    "            [3349,3447,15],[6128,6221,15],\n",
    "            [8475,8543,16],\n",
    "            [9792,9840,17],\n",
    "            [2984,3007,20],[4131,4183,20],[5025,5042,20],[6330,6355,20],[11818,11893,20],\n",
    "            [6760,6875,21],\n",
    "            [12871,12895,28],[13645,13667,28],[14232,14319,28],[14837,14849,28],[15521,15542,28],[16421,16445,28],[17032,17040,28]]\n",
    "custom_artifacts_6 = [[8863,9191,4],\n",
    "           [5389,5393,7],[5575,5607,7],[5754,5755,7],[5798,6000,7],\n",
    "            [11050,11070,10],[12260,12280,10],\n",
    "            [15125,15290,12],[16420,18160,10],\n",
    "            [13280,13340,13],\n",
    "            [8,23,15],[92,94,15],[481,505,15],\n",
    "            [10035,10500,20],[11024,11044,20],\n",
    "            [5794,5800,21],\n",
    "            [3501,3507,22]]\n",
    "custom_artifacts_7 = [[0,3418,0],[13116,13507,0],[14502,14872,0],\n",
    "              [8000,9000,2],\n",
    "              [1828,2638,3],[8631,12630,3],\n",
    "              [1497,1988,4],[16470,17270,4],\n",
    "              [5113,8239,5],                                                                                         \n",
    "              [2322,2978,6],\n",
    "              [12460,14890,7],\n",
    "              [15600,16060,10],\n",
    "              [4231,4848,11],[6434,6909,11],[9110,11550,11],\n",
    "              [11890,12380,12],\n",
    "              [8410,8820,13],\n",
    "              [3026,3515,14],[13810,14170,14],\n",
    "              [10910,12010,15]]  \n",
    "artifacts = [custom_artifacts_1, custom_artifacts_2, custom_artifacts_3,\n",
    "                custom_artifacts_4, custom_artifacts_5, custom_artifacts_6, custom_artifacts_7]\n",
    "\n",
    "# cell removal \n",
    "remove_cell_1 = []\n",
    "remove_cell_2 = []\n",
    "remove_cell_3 = [62,63,64,65,66,67,68,69,70,80,81,160,161,162,163,164,167,194,195,202,203,204]\n",
    "remove_cell_4 = [13,53,55,57,79,84,85,86,95,99,112,141,144,147,153,177,178,183,184,188,189,191,196,197,198,199,202,219,226,234,235]\n",
    "remove_cell_5 = []\n",
    "remove_cell_6 = [66,84,85,86]\n",
    "remove_cell_7 = []\n",
    "remove_cells = [remove_cell_1, remove_cell_2, remove_cell_3,\n",
    "              remove_cell_4, remove_cell_5, remove_cell_6, remove_cell_7]\n",
    "# mouse name\n",
    "miceName = ['250520_m18a', '290520_m17a', '110620_m18b', '050820_m21a', '030720_m20a', '291020_m23a', '210121_m24a']\n",
    "\n",
    "\n",
    "# saving figure \n",
    "save_fig = True # or False\n",
    "fig_save_add = ''\n",
    "\n",
    "\n",
    "# saving dataframes\n",
    "save_file = True # or False\n",
    "file_save_add = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d911b8",
   "metadata": {},
   "source": [
    "### Necessary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fbbe580",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dict(mouseName, unitNr, data, hypnoState, sf, apply_artifact, removeCell):\n",
    "    return {'mouseName':mouseName, \n",
    "            'unitNr':unitNr, \n",
    "            'data':data, \n",
    "            'hypnoState':hypnoState, \n",
    "            'samplingRate':sf, \n",
    "            'appliedArtifact':apply_artifact, \n",
    "            'removedCell':removeCell}\n",
    "\n",
    "def create_list(data, hypno, mouseName, sf, apply_artifact, removeCell):\n",
    "    \n",
    "    # initialize list\n",
    "    myList = []\n",
    "    \n",
    "    # looping over units\n",
    "    for i in range(hypno.shape[1]):\n",
    "        unit_seqs = []\n",
    "        unit_seqs = findseq(hypno[:,i])\n",
    "\n",
    "        for state, start, end, duration in zip(unit_seqs.state, unit_seqs.start_index, unit_seqs.end_index, unit_seqs.duration):\n",
    "            myList.append(create_dict(mouseName=mouseName, unitNr=i, \n",
    "                                      data=data[start:end + 1, :, i], \n",
    "                                      hypnoState=state, sf = sf, \n",
    "                                      apply_artifact=apply_artifact, \n",
    "                                      removeCell=removeCell))\n",
    "            \n",
    "    return myList\n",
    "\n",
    "def get_data(address, sf):\n",
    "    \n",
    "    # using ETL from Analysis file\n",
    "    data_load = ETL(sf = sf, ending='.mat', unit_length=600)\n",
    "    data_load.get_path(address)\n",
    "    data_load.load_files()\n",
    "\n",
    "    # get data values\n",
    "    data = data_load.get_data_values()\n",
    "    hypno = data_load.get_hypno()\n",
    "    \n",
    "    print(f'data and hypno shapes in file {address} are: {data.shape}, {hypno.shape}')\n",
    "    \n",
    "    return data, hypno\n",
    "\n",
    "def find_type2_locations(dictLists, pattern = [0,-2], cond1_min=100, cond1_max = 10000, cond2_min = 100, cond2_max = 10000):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function takes dictionary list as input and based on given pattern it returns \n",
    "    the locations in the list where that pattern happens. # Location of first element of the pattern.\n",
    "    \n",
    "    dictLists: List of dictionaries\n",
    "    pattern: pattern to search 0--> awake, -2-->SWS, -3-->REM\n",
    "    cond1_min: min epoch duration of the first element of the pattern\n",
    "    cond1_max: max epoch duration of the first element of the pattern\n",
    "    cond2_min: min epoch duration of the second element of the pattern\n",
    "    cond2_max: max epoch duration of the second element of the pattern\n",
    "    Hint: all condition will be calculated in sample based NOT in time based\n",
    "    \"\"\"\n",
    "    # first, reading all hypno states to a list\n",
    "    all_states = [myDict['hypnoState'] for myDict in dictLists]\n",
    "    \n",
    "    # second, control condition on each epoch based on condition on first and second elements of the pattern.\n",
    "    # first element\n",
    "    window_cond1 = [(myDict['epochDuration']>cond1_min and myDict['epochDuration']<cond1_max) for myDict in dictLists]\n",
    "    \n",
    "    # second element\n",
    "    window_cond2 = [(myDict['epochDuration']>cond2_min and myDict['epochDuration']<cond2_max) for myDict in dictLists]\n",
    "    \n",
    "    # reading all mouse names (needs to be checked to see if pattern comming from same mouse)\n",
    "    all_names = [myDict['mouseName'] for myDict in dictLists]\n",
    "    \n",
    "    # finding unique mouse name\n",
    "    # unique_name = np.unique(all_names)\n",
    "    \n",
    "    # finding the locations matching to a given pattern\n",
    "    locs = np.where((np.array(all_states[:-1]) == pattern[0]) & # first element of pattern\n",
    "                     (np.array(all_states[1:]) == pattern[1]) & # second element of the pattern\n",
    "                     (np.array(all_names[:-1] == np.array(all_names[1:]))) & # pattern comming from same mouse\n",
    "                     np.array(window_cond1[:-1]) & # condition on first element\n",
    "                     np.array(window_cond2[1:]))[0] # condition on second element\n",
    "    \n",
    "    return locs\n",
    "\n",
    "def find_type3_locations(dictLists, pattern = [-2,-3, -2], cond1_min=100, cond1_max = 10000, \n",
    "                         cond2_min = 100, cond2_max = 10000, cond3_min = 100, cond3_max = 10000):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function takes dictionary list as input and based on given pattern it returns \n",
    "    the locations in the list where that pattern happens. # Location of first element of the pattern.\n",
    "    \n",
    "    dictLists: List of dictionaries\n",
    "    pattern: pattern to search 0--> awake, -2-->SWS, -3-->REM\n",
    "    cond1_min: min epoch duration of the first element of the pattern\n",
    "    cond1_max: max epoch duration of the first element of the pattern\n",
    "    cond2_min: min epoch duration of the second element of the pattern\n",
    "    cond2_max: max epoch duration of the second element of the pattern\n",
    "    Hint: all condition will be calculated in sample based NOT in time based\n",
    "    \"\"\"\n",
    "    # first, reading all hypno states to a list\n",
    "    all_states = [myDict['hypnoState'] for myDict in dictLists]\n",
    "    \n",
    "    # second, control condition on each epoch based on condition on first and second elements of the pattern.\n",
    "    # first element\n",
    "    window_cond1 = [(myDict['epochDuration']>cond1_min and myDict['epochDuration']<cond1_max) for myDict in dictLists]\n",
    "    \n",
    "    # second element\n",
    "    window_cond2 = [(myDict['epochDuration']>cond2_min and myDict['epochDuration']<cond2_max) for myDict in dictLists]\n",
    "    \n",
    "    # third element\n",
    "    window_cond3 = [(myDict['epochDuration']>cond3_min and myDict['epochDuration']<cond3_max) for myDict in dictLists]\n",
    "    \n",
    "    # reading all mouse names (needs to be checked to see if pattern comming from same mouse)\n",
    "    all_names = [myDict['mouseName'] for myDict in dictLists]\n",
    "    \n",
    "    # finding unique mouse name\n",
    "    # unique_name = np.unique(all_names)\n",
    "    \n",
    "    # finding the locations matching to a given pattern\n",
    "    locs = np.where((np.array(all_states[:-1]) == pattern[0]) & # first element of pattern\n",
    "                     (np.array(all_states[1:]) == pattern[1]) & # second element of the pattern\n",
    "                     (np.array(all_names[:-1] == np.array(all_names[1:]))) & # pattern comming from same mouse\n",
    "                     np.array(window_cond1[:-1]) & # condition on first element\n",
    "                     np.array(window_cond2[1:]))[0] # condition on second element\n",
    "    \n",
    "    # getting all locations matching 3 pattern\n",
    "    temp_loc3 = []\n",
    "    temp_loc3 = np.where(np.array(all_states) == pattern[2])[0]\n",
    "    \n",
    "    out = []\n",
    "    \n",
    "    # searching for right candidates\n",
    "    for i, can in enumerate(locs + 1):\n",
    "        \n",
    "        temp = []\n",
    "        temp = np.where(temp_loc3 > can)[0] # all locations in pattern 3\n",
    "        \n",
    "        if temp.any() and all_names[temp_loc3[temp[0]] == all_names[can]] and window_cond3[temp_loc3[temp[0]]]:\n",
    "            out.append([can - 1, can, temp_loc3[temp[0]]])\n",
    "    \n",
    "    return np.vstack(out)\n",
    "\n",
    "def applyArtifacts(data, artifacts):\n",
    "    if artifacts:\n",
    "        for artifact in artifacts:\n",
    "            data[artifact[0]:artifact[1], :, artifact[2]] = 0\n",
    "        \n",
    "        return data\n",
    "    return data\n",
    "\n",
    "def removeCells(data, cells):\n",
    "    if cells:\n",
    "        return np.delete(data, cells, axis = 1)\n",
    "    return data\n",
    "\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "\n",
    "def avg_firing(data, sf):\n",
    "    # return in avgerage in second\n",
    "    avg = np.nansum(np.where(data>0 , 1, np.nan), axis = 0)/(data.shape[0]/sf)\n",
    "    return np.where(avg>0, avg, np.nan)\n",
    "\n",
    "def avg_amplitude(data):\n",
    "    # return in avgerage amplitude\n",
    "    return np.nanmean(np.where(data>0 , data, np.nan), axis = 0)\n",
    "\n",
    "def nrActiveCells(data):\n",
    "    return len(data[~np.isnan(data)])\n",
    "    \n",
    "def div_avg_firing(data, sf, n =3):\n",
    "    spaces = np.linspace(0, data.shape[0], n+1, dtype = int)\n",
    "    avg = [avg_firing(data=data[spaces[i]:spaces[i+1],:], sf=sf) for i in range(n)]\n",
    "    return avg\n",
    "    \n",
    "def div_avg_amplitude(data, n =3):\n",
    "    spaces = np.linspace(0, data.shape[0], n+1, dtype = int)\n",
    "    avg = [avg_amplitude(data=data[spaces[i]:spaces[i+1],:]) for i in range(n)]\n",
    "    return avg\n",
    "\n",
    "def top_least(data, prc = 10, top = True):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function return indices and values of top or least x% of given data\n",
    "    \n",
    "    data: array\n",
    "    prc: percentage\n",
    "    top: if True returns top if false returns least\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    \n",
    "    indx = np.argsort(data)\n",
    "    \n",
    "    notNans = ~np.isnan(data[indx])\n",
    "    \n",
    "    selc = max(1, int(len(data) * (prc/100)))\n",
    "    \n",
    "    if top:\n",
    "        return indx[notNans][-selc:], data[indx[notNans]][-selc:]\n",
    "    \n",
    "    if not top:\n",
    "        return indx[notNans][:selc], data[indx[notNans]][:selc]\n",
    "    \n",
    "    \n",
    "\n",
    "def cellID(indices, mouse_name, all_mice_names):\n",
    "    \n",
    "    '''\n",
    "    This function return explicit cell ID based on list of indices and mouse name\n",
    "    '''\n",
    "    \n",
    "    all_mice_names = np.unique(all_mice_names)\n",
    "    \n",
    "    Dict = {}\n",
    "    for i, name in enumerate(all_mice_names):\n",
    "        Dict.update({name: (i+1) * 1000})\n",
    "    \n",
    "    return  indices + Dict[mouse_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe444fe",
   "metadata": {},
   "source": [
    "### Creating base data structure and loading data and hypno to it\n",
    "####    If ncessary first apply artifact on data or remove cells then create data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb646655",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your entered path is L:/analysis_nima/250520_m18a/Batch_4\n",
      "All files in the given path \n",
      "\n",
      "['unit01.mat', 'unit02.mat', 'unit03.mat', 'unit04.mat', 'unit05.mat', 'unit06.mat', 'unit07.mat', 'unit08.mat', 'unit09.mat', 'unit10.mat', 'unit11.mat', 'unit12.mat', 'unit13.mat', 'unit14.mat', 'unit15.mat', 'unit16.mat', 'unit17.mat', 'unit18.mat', 'unit19.mat', 'unit20.mat', 'unit21.mat', 'unit22.mat', 'unit23.mat', 'unit24.mat', 'unit25.mat', 'unit26.mat', 'unit27.mat', 'unit28.mat', 'unit29.mat', 'unit30.mat', 'unit31.mat', 'unit32.mat', 'unit33.mat', 'unit34.mat', 'unit35.mat', 'unit36.mat', 'unit37.mat', 'unit38.mat', 'unit39.mat', 'unit40.mat', 'unit41.mat', 'unit42.mat', 'unit43.mat', 'unit44.mat', 'unit45.mat']  Number of all files 45\n",
      "Data are loaded \n",
      "\n",
      "Start reading units \n",
      "\n",
      "unit length is 600 sec, data length is 18597 samples, number of cells are 236 \n",
      "\n",
      "Finished reading units \n",
      "\n",
      "Start reading hypno files \n",
      "\n",
      "Finished reading hypno \n",
      "\n",
      "Location-Time information\n",
      "data and hypno shapes in file L:/analysis_nima/250520_m18a/Batch_4 are: (18597, 236, 45), (18597, 45)\n",
      "Your entered path is L:/analysis_nima/290520_m17a/Batch/extracells/Batch_2\n",
      "All files in the given path \n",
      "\n",
      "['unit01.mat', 'unit02.mat', 'unit03.mat', 'unit04.mat', 'unit05.mat', 'unit06.mat', 'unit07.mat', 'unit08.mat', 'unit09.mat', 'unit10.mat', 'unit11.mat', 'unit12.mat', 'unit13.mat', 'unit14.mat', 'unit15.mat', 'unit16.mat', 'unit17.mat', 'unit18.mat', 'unit19.mat', 'unit20.mat', 'unit21.mat', 'unit22.mat', 'unit23.mat', 'unit24.mat', 'unit25.mat', 'unit26.mat', 'unit27.mat', 'unit28.mat', 'unit29.mat', 'unit30.mat', 'unit31.mat']  Number of all files 31\n",
      "Data are loaded \n",
      "\n",
      "Start reading units \n",
      "\n",
      "unit length is 600 sec, data length is 18597 samples, number of cells are 294 \n",
      "\n",
      "Finished reading units \n",
      "\n",
      "Start reading hypno files \n",
      "\n",
      "Finished reading hypno \n",
      "\n",
      "Location-Time information\n",
      "data and hypno shapes in file L:/analysis_nima/290520_m17a/Batch/extracells/Batch_2 are: (18597, 294, 31), (18597, 31)\n",
      "Your entered path is L:/analysis_nima/110620_m18b/new/Batch_3\n",
      "All files in the given path \n",
      "\n",
      "['unit01.mat', 'unit02.mat', 'unit03.mat', 'unit04.mat', 'unit05.mat', 'unit06.mat', 'unit07.mat', 'unit08.mat', 'unit09.mat', 'unit10.mat', 'unit11.mat', 'unit12.mat', 'unit13.mat', 'unit14.mat', 'unit15.mat', 'unit16.mat', 'unit17.mat', 'unit18.mat', 'unit19.mat', 'unit20.mat', 'unit21.mat', 'unit22.mat', 'unit23.mat', 'unit24.mat', 'unit25.mat', 'unit26.mat']  Number of all files 26\n",
      "Data are loaded \n",
      "\n",
      "Start reading units \n",
      "\n",
      "unit length is 600 sec, data length is 18573 samples, number of cells are 218 \n",
      "\n",
      "Finished reading units \n",
      "\n",
      "Start reading hypno files \n",
      "\n",
      "Finished reading hypno \n",
      "\n",
      "Location-Time information\n",
      "data and hypno shapes in file L:/analysis_nima/110620_m18b/new/Batch_3 are: (18573, 218, 26), (18573, 26)\n",
      "Your entered path is L:/analysis_nima/050820_m21a/Batch_4\n",
      "All files in the given path \n",
      "\n",
      "['unit01.mat', 'unit02.mat', 'unit03.mat', 'unit04.mat', 'unit05.mat', 'unit06.mat', 'unit07.mat', 'unit08.mat', 'unit09.mat', 'unit10.mat', 'unit11.mat', 'unit12.mat', 'unit13.mat', 'unit14.mat', 'unit15.mat', 'unit16.mat', 'unit17.mat', 'unit18.mat', 'unit19.mat', 'unit20.mat', 'unit21.mat', 'unit22.mat', 'unit23.mat', 'unit24.mat', 'unit25.mat', 'unit26.mat', 'unit27.mat', 'unit28.mat', 'unit29.mat', 'unit30.mat', 'unit31.mat', 'unit32.mat', 'unit33.mat', 'unit34.mat', 'unit35.mat', 'unit36.mat', 'unit37.mat', 'unit38.mat', 'unit39.mat', 'unit40.mat', 'unit41.mat', 'unit42.mat', 'unit43.mat', 'unit44.mat', 'unit45.mat', 'unit46.mat']  Number of all files 46\n",
      "Data are loaded \n",
      "\n",
      "Start reading units \n",
      "\n",
      "unit length is 600 sec, data length is 18597 samples, number of cells are 236 \n",
      "\n",
      "Finished reading units \n",
      "\n",
      "Start reading hypno files \n",
      "\n",
      "Finished reading hypno \n",
      "\n",
      "Location-Time information\n",
      "data and hypno shapes in file L:/analysis_nima/050820_m21a/Batch_4 are: (18597, 236, 46), (18597, 46)\n",
      "Your entered path is L:/analysis_nima/030720_m20a/Batch_3\n",
      "All files in the given path \n",
      "\n",
      "['unit01.mat', 'unit02.mat', 'unit03.mat', 'unit04.mat', 'unit05.mat', 'unit06.mat', 'unit07.mat', 'unit08.mat', 'unit09.mat', 'unit10.mat', 'unit11.mat', 'unit12.mat', 'unit13.mat', 'unit14.mat', 'unit15.mat', 'unit16.mat', 'unit17.mat', 'unit18.mat', 'unit19.mat', 'unit20.mat', 'unit21.mat', 'unit22.mat', 'unit23.mat', 'unit24.mat', 'unit25.mat', 'unit26.mat', 'unit27.mat', 'unit28.mat', 'unit29.mat']  Number of all files 29\n",
      "Data are loaded \n",
      "\n",
      "Start reading units \n",
      "\n",
      "unit length is 600 sec, data length is 18597 samples, number of cells are 185 \n",
      "\n",
      "Finished reading units \n",
      "\n",
      "Start reading hypno files \n",
      "\n",
      "Finished reading hypno \n",
      "\n",
      "Location-Time information\n",
      "data and hypno shapes in file L:/analysis_nima/030720_m20a/Batch_3 are: (18597, 185, 29), (18597, 29)\n",
      "Your entered path is L:/analysis_nima/291020_m23a/Batch_2\n",
      "All files in the given path \n",
      "\n",
      "['unit01.mat', 'unit02.mat', 'unit03.mat', 'unit04.mat', 'unit05.mat', 'unit06.mat', 'unit07.mat', 'unit08.mat', 'unit09.mat', 'unit10.mat', 'unit11.mat', 'unit12.mat', 'unit13.mat', 'unit14.mat', 'unit15.mat', 'unit16.mat', 'unit17.mat', 'unit18.mat', 'unit19.mat', 'unit20.mat', 'unit21.mat', 'unit22.mat', 'unit23.mat']  Number of all files 23\n",
      "Data are loaded \n",
      "\n",
      "Start reading units \n",
      "\n",
      "unit length is 600 sec, data length is 18574 samples, number of cells are 87 \n",
      "\n",
      "Finished reading units \n",
      "\n",
      "Start reading hypno files \n",
      "\n",
      "Finished reading hypno \n",
      "\n",
      "Location-Time information\n",
      "data and hypno shapes in file L:/analysis_nima/291020_m23a/Batch_2 are: (18574, 87, 23), (18574, 23)\n",
      "Your entered path is L:/analysis_nima/210121_m24a/Batch_1\n",
      "All files in the given path \n",
      "\n",
      "['unit01.mat', 'unit02.mat', 'unit03.mat', 'unit04.mat', 'unit05.mat', 'unit06.mat', 'unit07.mat', 'unit08.mat', 'unit09.mat', 'unit10.mat', 'unit11.mat', 'unit12.mat', 'unit13.mat', 'unit14.mat', 'unit15.mat', 'unit16.mat', 'unit17.mat', 'unit18.mat', 'unit19.mat', 'unit20.mat', 'unit21.mat', 'unit22.mat', 'unit23.mat', 'unit24.mat', 'unit25.mat', 'unit26.mat', 'unit27.mat']  Number of all files 27\n",
      "Data are loaded \n",
      "\n",
      "Start reading units \n",
      "\n",
      "unit length is 600 sec, data length is 18597 samples, number of cells are 143 \n",
      "\n",
      "Finished reading units \n",
      "\n",
      "Start reading hypno files \n",
      "\n",
      "Finished reading hypno \n",
      "\n",
      "Location-Time information\n",
      "data and hypno shapes in file L:/analysis_nima/210121_m24a/Batch_1 are: (18597, 143, 27), (18597, 27)\n"
     ]
    }
   ],
   "source": [
    "apply_artifact = True\n",
    "rmCell = True\n",
    "\n",
    "data_list = []\n",
    "for address, sf, mouse_name, artifact, remove_cell in zip(all_address, sfs, miceName, artifacts, remove_cells):\n",
    "    \n",
    "    #clear and read data\n",
    "    data = []; hypno = []\n",
    "    data, hypno = get_data(address = address, sf = sf)\n",
    "    \n",
    "    if apply_artifact:\n",
    "        data = applyArtifacts(data=data, artifacts=artifact)\n",
    "    \n",
    "    if rmCell:\n",
    "        data = removeCells(data=data, cells = remove_cell)\n",
    "    \n",
    "    # creat per mouse list\n",
    "    data_list.extend(create_list(data = data, hypno = hypno, mouseName = mouse_name, \n",
    "                                 sf = sf, apply_artifact=apply_artifact, removeCell=rmCell))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e182274",
   "metadata": {},
   "source": [
    "### Adding analyses as dictionary key, value pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0580b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add average firing rate to the dictionary\n",
    "[data_dict.update({'avg_firing_perSecond':avg_firing(data = data_dict['data'], \n",
    "                                                            sf=data_dict['samplingRate'])}) for data_dict in data_list];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cf41d93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:167: RuntimeWarning: Mean of empty slice\n"
     ]
    }
   ],
   "source": [
    "# add average amplitude to the dictionary\n",
    "[data_dict.update({'avg_amplitude':avg_amplitude(data = data_dict['data'])}) for data_dict in data_list];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7df54cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add top and least firing rate \n",
    "fraction = 20\n",
    "\n",
    "#top\n",
    "[data_dict.update({'top' + str(fraction) + '_avg_firing':top_least(data=data_dict['avg_firing_perSecond'], \n",
    "                                                                   prc = fraction, top = True)}) for data_dict in data_list];\n",
    "\n",
    "#least\n",
    "[data_dict.update({'least' + str(fraction) + '_avg_firing':top_least(data=data_dict['avg_firing_perSecond'], \n",
    "                                                                   prc = fraction, top = False)}) for data_dict in data_list];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd23d009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add top and least amplitude \n",
    "fraction = 20\n",
    "\n",
    "#top\n",
    "[data_dict.update({'top' + str(fraction) + '_amplitude':top_least(data=data_dict['avg_amplitude'], \n",
    "                                                                   prc = fraction, top = True)}) for data_dict in data_list];\n",
    "\n",
    "#least\n",
    "[data_dict.update({'least' + str(fraction) + '_amplitude':top_least(data=data_dict['avg_amplitude'], \n",
    "                                                                   prc = fraction, top = False)}) for data_dict in data_list];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27884385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add divided average firing rate to the dictionary\n",
    "[data_dict.update({'div_avg_firing_perSecond':div_avg_firing(data = data_dict['data'], \n",
    "                                                             sf=data_dict['samplingRate'], \n",
    "                                                             n = 3)}) for data_dict in data_list];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1d0a3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:167: RuntimeWarning: Mean of empty slice\n"
     ]
    }
   ],
   "source": [
    "# add divided average firing rate to the dictionary (n subdata)\n",
    "[data_dict.update({'div_avg_amplitude':div_avg_amplitude(data = data_dict['data'], n=3)}) for data_dict in data_list];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f4f8d0f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# add epoch duration in sample\n",
    "[data_dict.update({'epochDuration': data_dict['data'].shape[0]}) for data_dict in data_list];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8fa4f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add nr of active cell per epoch\n",
    "[data_dict.update({'nrActiveCells': nrActiveCells(data = data_dict['avg_amplitude'])}) for data_dict in data_list];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08ffca81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add nr of total cells per epoch\n",
    "[data_dict.update({'nrTotalCells': data_dict['data'].shape[1]}) for data_dict in data_list];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "907031bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding location of type2 pattern to the dictionaries\n",
    "\n",
    "\n",
    "# type2: 1 epoch is first element of the pattern\n",
    "#        2 epoch is second element of the pattern\n",
    "#        0 epoch is not involved in the pattern\n",
    "\n",
    "\n",
    "# finding locations (each location is the location of first element of the pattern)\n",
    "type2_locations = find_type2_locations(dictLists=data_list, pattern=[-2, -3], \n",
    "                                       cond1_min= 900, cond1_max=10000, \n",
    "                                       cond2_min=900, cond2_max=10000)\n",
    "\n",
    "\n",
    "\n",
    "[data_dict.update({'type2': 0}) for data_dict in data_list]\n",
    "\n",
    "for i, data_dict in enumerate(data_list):\n",
    "    if i in type2_locations:\n",
    "        data_dict.update({'type2':1})\n",
    "        data_list[i+1].update({'type2':2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "a1ce4be9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# adding location of type3 pattern to the dictionaries\n",
    "\n",
    "\n",
    "# type2: 1 epoch is first element of the pattern\n",
    "#        2 epoch is second element of the pattern\n",
    "#        3 epoch is third element of the pattern\n",
    "#        0 epoch is not involved in the pattern\n",
    "\n",
    "\n",
    "# finding locations (each location is the location of first element of the pattern)\n",
    "type3_locations = find_type3_locations(dictLists=data_list, pattern=[-2, -3, -2], \n",
    "                                       cond1_min=900, cond1_max=10000, \n",
    "                                       cond2_min=900, cond2_max=10000, \n",
    "                                       cond3_min=900, cond3_max=10000)\n",
    "\n",
    "\n",
    "\n",
    "[data_dict.update({'type3': 0}) for data_dict in data_list]\n",
    "\n",
    "for i in range(type3_locations.shape[0]):\n",
    "    \n",
    "    # put all pattern locations in the first element of the pattern\n",
    "    data_list[type3_locations[i,0]].update({'type3':type3_locations[i,:]})\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7796dc",
   "metadata": {},
   "source": [
    "# Plotting of analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498bd421",
   "metadata": {},
   "source": [
    "## Doublets TO Triplets overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "7a182e1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Doubplets are: 94\n",
      "Number of Triplets are: 79\n",
      "Triplets TO Doubplets overlap is 84 %\n"
     ]
    }
   ],
   "source": [
    "Doublets = 0\n",
    "Triplets = 0\n",
    "Doublet_Triplets = 0\n",
    "\n",
    "for i, indDict in enumerate(data_list):\n",
    "    if not isinstance(indDict['type3'], int):\n",
    "        Triplets += 1\n",
    "        \n",
    "    if indDict[\"type2\"] == 1:\n",
    "        Doublets += 1\n",
    "        \n",
    "    if (not isinstance(indDict['type3'], int)) and (indDict[\"type2\"] == 1):\n",
    "        Doublet_Triplets += 1\n",
    "        \n",
    "assert Doublets >= Triplets\n",
    "assert Doublets >= Doublet_Triplets\n",
    "assert Triplets >= Doublet_Triplets\n",
    "\n",
    "print(f\"Number of Doubplets are: {Doublets}\\nNumber of Triplets are: {Triplets}\\nTriplets TO Doubplets overlap is {int(((Doublet_Triplets) / Doublets) * 100)} %\")\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
